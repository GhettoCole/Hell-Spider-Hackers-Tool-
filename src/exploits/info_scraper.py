import re
import logging
import os
import requests
from bs4 import BeautifulSoup
from urllib.request import urlopen
from urllib.request import urlretrieve
from Webcrawler import WebCrawler

logging.basicConfig(filename="scrape.log", level=logging.DEBUG)

colors = {
    'BLUE': '\33[1;94m',
    'RED': '\033[1;91m',
    'WHITE': '\33[1;97m',
    'YELLOW': '\33[1;93m',
    'MAGENTA': '\033[1;35m',
    'GREEN': '\033[1;32m',
    'END': '\033[0m',
}


class ScraperInfo(WebCrawler):

    global colors

    def __init__(self, base_url):
        super().__init__(base_url)

    def sourceCode(self, name="index"):
        directory = str(os.getcwd())
        request = urlopen(self.base_url)

        # bs4 object
        bsObj = BeautifulSoup(request, "lxml")
        htmlCode = bsObj.prettify()

        file = "{}.html".format(name)

        with open(directory+file, "w") as f:
            f.write(htmlCode)
        f.close()

        print(colors["RED"], "Your file has been saved in {} as {}".format(directory, file), colors["END"])

    def HTTP_headers(self):
        request = urlopen(self.base_url)
        # HTTP headers
        TAB = "\t" * 5
        print(colors["YELLOW"], TAB, request.info(), colors["END"])

    def siteImages(self):
        links = set()
        request = urlopen(self.base_url)
        bsObj = BeautifulSoup(request, "lxml")
        for link in bsObj.find_all('a'):
            if 'src' in link.attrs:
                # image has source
                downloadURL = link.attrs['src']
                print("[+] Storing => {}".format(downloadURL))
                links.add(downloadURL)
            else:
                pass

        index = 0
        name = "{} .jpg".format(str(index))
        # directory = os.getcwd()
        try:
            for link in links:
                urlretrieve(link, name)
                index += 1
        except Exception as e:
            print(colors["RED"], "Error => ", e, colors["END"])

    def commentHarvester(self):
        """Harvests comments on pages for later analysis"""
        urls = []
        url = requests.get(self.base_url)
        # look for any comment
        comments = re.findall("<!---(.*)-->", url.text)
        print("Comments found on {}".format(self.base_url))
        for comment in comments:
            # I just like being sure
            if comment is not None:
                print(colors["BLUE"], comment, colors["END"])
            else:
                pass
        # change to HTML.parser if need be
        bsOBj = BeautifulSoup(url.text, "lxml")
        for link in bsOBj.find_all('a'):
            if 'href' in link.attrs:
                urls.append(link)
                # newLink = link.get('href')
                # try:
                #     if url in newLink:
                #         urls.append(str(newLink))
                #     elif link.attrs[-1] == "/":
                #         coJoint = url+newLink
                #         urls.append(str(coJoint))
                # except Exception as e:
                #     print(colors["RED"], "Error ", e, colors["END"])
        # for link in urls:
        #     print(colors["GREEN"], "Harvesting more links on {}".format(url), colors["END"])
        #     url = requests.get(link)
        comments = re.findall("<!--(.*)-->", bsOBj.text)
        for comment in comments:
            print(colors["YELLOW"], comment, colors["END"])
